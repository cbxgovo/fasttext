# coding: UTF-8
import time
import torch
import numpy as np
from train_eval import train, init_network
from importlib import import_module
# from utils_fasttext import build_dataset, build_iterator, get_time_dif
import argparse
import pickle as pkl
import pickle


class NewsClassifier:
    def __init__(self):
        print("NewsClassifier initializing!")
        self.parser = argparse.ArgumentParser(description='Chinese Text Classification')
        self.parser.add_argument('--model', default='FastText', type=str)
        self.parser.add_argument('--embedding', default='random', type=str)

        self.args = self.parser.parse_args()
        self.args.word = False
        self.dataset = 'THUCNews' # ä¿®æ”¹ æ ¹åœ°å€ ä¼ åˆ°æ¨¡å‹config å¯»æ‰¾ä¿å­˜çš„è¯è¡¨ æ¨¡å‹æ–‡ä»¶ç­‰
        self.embedding = 'random'
        self.model_name = self.args.model
        self.labels = ["0", "1", "2","3","4","5"] # ä¿®æ”¹ æ•°æ®é›†é¢„æµ‹æ ‡ç­¾ å’Œè®­ç»ƒçš„class.txté¡ºåºå¯¹åº”

        x = import_module('models.' + self.model_name)
        self.config = x.Config(self.dataset, self.embedding)

        # åŠ é€Ÿä¼˜åŒ–ï¼šå–æ¶ˆ cudnn deterministicï¼ˆè‹¥æ¨ç†ç»“æœå…è®¸æœ‰å¾®å·®ï¼‰
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

        np.random.seed(1)
        torch.manual_seed(1)
        torch.cuda.manual_seed_all(1)

        print("Loading data...")
        with open(self.config.vocab_path, 'rb') as f: # åŠ è½½å·²æœ‰çš„è¯è¡¨ å¦‚æœæœ‰çš„è¯
            self.vocab = pickle.load(f)
        self.config.n_vocab = len(self.vocab)

        print("Loading model...")
        self.model = x.Model(self.config).to(self.config.device)
        if self.model_name != 'Transformer':
            init_network(self.model)
        self.model.load_state_dict(torch.load(self.config.save_path, map_location=self.config.device))
        self.model.eval()
        print("Model ready.")

    def my_to_tensor(self, config, datas):
        x = torch.LongTensor([_[0] for _ in datas]).to(config.device)
        y = torch.LongTensor([_[1] for _ in datas]).to(config.device)
        bigram = torch.LongTensor([_[3] for _ in datas]).to(config.device)
        trigram = torch.LongTensor([_[4] for _ in datas]).to(config.device)
        seq_len = torch.LongTensor([_[2] for _ in datas]).to(config.device)
        return (x, seq_len, bigram, trigram)

    def str2numpy(self, text, config):
        UNK, PAD = '<UNK>', '<PAD>'
        tokenizer = lambda x: [y for y in x]  # char-level
        vocab = self.vocab

        def biGramHash(sequence, t, buckets):
            t1 = sequence[t - 1] if t - 1 >= 0 else 0
            return (t1 * 14918087) % buckets

        def triGramHash(sequence, t, buckets):
            t1 = sequence[t - 1] if t - 1 >= 0 else 0
            t2 = sequence[t - 2] if t - 2 >= 0 else 0
            return (t2 * 14918087 * 18408749 + t1 * 14918087) % buckets

        def to_numpy(content, pad_size=32):
            token = tokenizer(content)
            seq_len = min(len(token), pad_size)
            token = token[:pad_size] + [PAD] * max(0, pad_size - len(token))
            words_line = [vocab.get(word, vocab.get(UNK)) for word in token]
            buckets = config.n_gram_vocab
            bigram = [biGramHash(words_line, i, buckets) for i in range(pad_size)]
            trigram = [triGramHash(words_line, i, buckets) for i in range(pad_size)]
            return [(words_line, -1, seq_len, bigram, trigram)]

        npy = to_numpy(text, config.pad_size)
        return self.my_to_tensor(config, npy)

    def classify(self, title):
        with torch.no_grad():
            data = self.str2numpy(title, self.config)
            outputs = self.model(data)
            predic = torch.max(outputs.data, 1)[1].cpu().numpy()[0]
            return self.labels[predic]


if __name__ == '__main__':
    import os
    import time
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from tqdm import tqdm

    # input_file = 'THUCNews/data/0604.txt' # ä¿®æ”¹ éœ€è¦é¢„æµ‹çš„æ•°æ®é›†åœ°å€
    # output_file = 'THUCNews/data/0604_pre.txt'
    input_file = '/workspace/xumh3@xiaopeng.com/text_quality/output/train.txt' # ä¿®æ”¹ éœ€è¦é¢„æµ‹çš„æ•°æ®é›†åœ°å€ å½“å‰åœ°å€17389æ¡æµ‹è¯•æ•°æ®é›†
    output_file = '/workspace/xumh3@xiaopeng.com/text_quality/output/train_predictions.txt'  # ä¿®æ”¹ é¢„æµ‹ç»“æœä¿å­˜åœ°å€

    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        exit()

    classifier = NewsClassifier()

    with open(input_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]

    total_lines = len(lines)
    print(f"ğŸ“„ å¾…é¢„æµ‹æ–‡æœ¬æ€»æ•°ï¼š{total_lines} æ¡")

    results = [None] * total_lines
    num_threads = min(8, os.cpu_count())

    print("ğŸš€ å¼€å§‹å¤šçº¿ç¨‹å¹¶è¡Œé¢„æµ‹...")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        future_to_index = {
            executor.submit(classifier.classify, line): idx
            for idx, line in enumerate(lines)
        }

        for future in tqdm(as_completed(future_to_index), total=total_lines, desc="é¢„æµ‹ä¸­", ncols=80):
            idx = future_to_index[future]
            try:
                pred_label = future.result()
                results[idx] = (lines[idx], pred_label)
            except Exception as e:
                print(f"âŒ ç¬¬ {idx} è¡Œé¢„æµ‹å¤±è´¥ï¼š{e}")
                results[idx] = (lines[idx], "ERROR")

    with open(output_file, 'w', encoding='utf-8') as f_out:
        for text, label in results:
            f_out.write(f"{text}\t{label}\n")

    end_time = time.time()
    print(f"\nâœ… æ‰€æœ‰é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°ï¼š{output_file}")
    print(f"â±ï¸ æ€»è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")
