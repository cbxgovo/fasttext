# coding: UTF-8
import time
import torch
import numpy as np
from train_eval import train, init_network
from importlib import import_module
import argparse
import os
import pickle

parser = argparse.ArgumentParser(description='Chinese Text Classification')
parser.add_argument('--model', type=str, required=True, help='choose a model: TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer')
parser.add_argument('--embedding', default='pre_trained', type=str, help='random or pre_trained')
# é»˜è®¤æŒ‰ç…§char-levelè¿›è¡Œåˆ†è¯ï¼Œå…·ä½“è§utils_fasttext.pyä¸­çš„build_dataset
parser.add_argument('--word', default=False, type=bool, help='True for word, False for char')
args = parser.parse_args()

if __name__ == '__main__':
    dataset = 'THUCNews'  # æ•°æ®é›† æ ¹åœ°å€
    embedding = 'embedding_SougouNews.npz'
    if args.embedding == 'random':
        embedding = 'random'

    model_name = args.model
    if model_name == 'FastText':
        from utils_fasttext import build_dataset, build_iterator, get_time_dif
        embedding = 'random'
    else:
        from utils import build_dataset, build_iterator, get_time_dif

    x = import_module('models.' + model_name)
    config = x.Config(dataset, embedding)

    # ä¿è¯ç»“æœå¯å¤ç°
    np.random.seed(1)
    torch.manual_seed(1)
    torch.cuda.manual_seed_all(1)
    torch.backends.cudnn.deterministic = True

    print("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®...")

    # ä½¿ç”¨å›ºå®šè·¯å¾„ä½œä¸ºç¼“å­˜ä½ç½®
    vocab_path = os.path.join(dataset, 'data', 'vocab.pkl')
    data_cache_path = os.path.join(dataset, 'data', 'data_cache.pkl')
    os.makedirs(os.path.dirname(vocab_path), exist_ok=True)

    if os.path.exists(vocab_path) and os.path.exists(data_cache_path):
        print(f"âœ… æ£€æµ‹åˆ°ç¼“å­˜ï¼ŒåŠ è½½ {vocab_path} å’Œ {data_cache_path}")
        with open(vocab_path, 'rb') as f:
            vocab = pickle.load(f)
        with open(data_cache_path, 'rb') as f:
            train_data, dev_data, test_data = pickle.load(f)
    else:
        print("ğŸ“¦ æœªæ£€æµ‹åˆ°ç¼“å­˜ï¼Œé‡æ–°æ„å»º vocab å’Œæ•°æ®é›†")
        # args.wordé»˜è®¤æ˜¯char-level
        vocab, train_data, dev_data, test_data = build_dataset(config, args.word)
        with open(vocab_path, 'wb') as f:
            pickle.dump(vocab, f)
        with open(data_cache_path, 'wb') as f:
            pickle.dump((train_data, dev_data, test_data), f)

    train_iter = build_iterator(train_data, config)
    dev_iter = build_iterator(dev_data, config)
    test_iter = build_iterator(test_data, config)

    print("âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print("â±ï¸ è®¡æ—¶å¼€å§‹...")
    start_time = time.time()

    # train
    config.n_vocab = len(vocab)
    model = x.Model(config).to(config.device)
    if model_name != 'Transformer':
        init_network(model)
    print(model.parameters)
    train(config, model, train_iter, dev_iter, test_iter)

    time_dif = get_time_dif(start_time)
    print("â³ æ€»ç”¨æ—¶:", time_dif)
